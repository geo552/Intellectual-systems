{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Классификация\n",
    "================\n",
    "\n",
    "**Цель работы**: Изучить основные понятия, связанные с классификацией, научиться применять библиотеку scikit-learn, применить полученные знания для решения практических задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка библиотек\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.optimize as so\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc('font', family='Verdana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация\n",
    "-------------\n",
    "\n",
    "Задача классификации — это задача восстановления связи между объектами (описываемыми некоторым набором признаков) и конечным набором классов (описываемых номерами — метками).\n",
    "\n",
    "Эта задача широко распространена. Вот лишь несколько примеров:\n",
    "\n",
    "- выявление спама,\n",
    "- распознавание голосовых команд,\n",
    "- биометрическая идентификация,\n",
    "- медицинская диагностика,\n",
    "- выявление эмоциональной окраски текста,\n",
    "- разпознавание текста на изображениях и т. д.\n",
    "\n",
    "Если классов два, то говорят о бинарной классификации. Если больше, то о многоклассовой классификации. Многоклассовую классификацию всегда можно свести к бинарной, если совместно использовать для каждого класса бинарный классификатор, «реагирующий» только на него.\n",
    "\n",
    "Бинарный классификатор в идеале должен возвращать либо $0$, либо $1$. Но часто удобнее возвращать некоторую оценку из отрезка $[0, 1]$. Её можно трактовать как вероятность, что объект относится к классу $1$. Для объекта $x$, относящегося к классу $y$:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\textsf{P}[y = 1]\n",
    "$$\n",
    "\n",
    "Перейти к бинарным классам можно сравнивая предсказанную величину с порогом $\\frac12$. Если вероятность больше $\\frac12$, то предсказываем $1$, иначе — $0$.\n",
    "\n",
    "Задача классификации — это задача обучения с учителем. Классификацию без учителя называют кластеризацией.\n",
    "\n",
    "Существует много методов для решения этой задачи:\n",
    "\n",
    "- логистическая регрессия,\n",
    "- метод k ближайших соседей (kNN),\n",
    "- метод опорных векторов (SVM),\n",
    "- деревья решений,\n",
    "- икусственные нейронные сети и др."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия\n",
    "-----------------------\n",
    "\n",
    "Логистическая регрессия несмотря на название является методом классификации, а не регрессии. Для предсказания в ней используется следующая модель:\n",
    "$$\n",
    "h_{\\theta}(x) = \\sigma(\\theta^T x),\n",
    "$$\n",
    "где $\\sigma(z)$ — так называемая сигмоида:\n",
    "$$\n",
    "\\sigma(z) = \\frac1{1+e^{-z}}.\n",
    "$$\n",
    "\n",
    "Построим график сигмоиды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def s(z):\n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "y = s(x)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('s(x)')\n",
    "plt.title('График сигмоиды')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве функционала качества в логистической регрессии для улучшения сходимости лучше использовать не среднеквадратическую ошибку (MSE), а кросс-энтропию (cross-entropy, её ещё называют log-loss).\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac1{2m} \\sum_{i=1}^m \\Big[ \n",
    "    y^{(i)} \\ln \\sigma (\\theta^T x^{(i)}) + (1 - y^{(i)}) \\ln \\big(1 - \\sigma (\\theta^T x^{(i)})\\big)\n",
    "\\Big].\n",
    "$$\n",
    "\n",
    "Это выражение можно получить с помощью метода максимального правдоподобия, но его легко понять и интуитивно.\n",
    "\n",
    "Рассмотрим случай одного прецедента. Если $y = 1$, «работает» только первое слагаемое под суммой. Оно тем больше (с учётом минуса перед суммой), чем ближе предсказанное значение (то есть значение функции $\\sigma$) к 0. Если же оно равно 1, то слагаемое равно нулю. То есть, чем больше предсказанное значение отличается от $y$, тем больше всё выражение. Аналогичные рассуждения можно провести и для $y=0$.\n",
    "\n",
    "Полезной особенностью такого функционала качества является то, что выражение для его градиента формально совпадает с выражением градента линейной регрессии с точностью до $h_{\\theta}(x)$.\n",
    "$$\n",
    "\\big[\\operatorname{grad} J(\\theta)\\big]_j = \\frac1m \\sum_{i=1}^m \\big(h_{\\theta}(x^{(i)})-y^{(i)}\\big)x_j^{(i)}.\n",
    "$$\n",
    "\n",
    "Или в векторном виде:\n",
    "$$\n",
    "\\operatorname{grad} J(\\theta) = \\frac1m X^T (h_{\\theta}(X) - y).\n",
    "$$\n",
    "\n",
    "Таким образом программу для линейной регрессии легко преобразовать в программу для классификации методом логистической регрессии просто заменив модель $h_{\\theta}(x)$.\n",
    "\n",
    "Для борьбы с переобученностью как и в случае линейной регрессии добавляют «штраф» за большие значения коэффициентов $\\theta_j$ (кроме $\\theta_0$) в виде регуляризационного слагаемого:\n",
    "$$\n",
    "J(\\theta) = - \\frac1{2m} \\sum_{i=1}^m \\Big[ \n",
    "    y^{(i)} \\ln \\sigma (\\theta^T x^{(i)}) + (1 - y^{(i)}) \\ln \\big(1 - \\sigma (\\theta^T x^{(i)})\\big)\n",
    "\\Big] + \\frac{\\lambda}{2m} \\sum_{j=1}^n\\theta_j^2;\n",
    "$$\n",
    "$$\n",
    "\\big[\\operatorname{grad} J(\\theta)\\big]_j = \\frac1m \\sum_{i=1}^m \\big(h_{\\theta}(x^{(i)})-y^{(i)}\\big)x_j^{(i)} + \\frac{\\lambda}{m} \\sum_{j=1}^n\\theta_j.\n",
    "$$\n",
    "\n",
    "Увеличение $\\lambda$ уменьшает переобученность, но и снижает точность. При $\\lambda = 0$ регуляризации нет вообще.\n",
    "\n",
    "Если регуляризационное слагаемое имеет вид $\\frac{\\lambda}{2m} \\sum_{j=1}^n\\theta_j^2$, то этот метод называется регуляризацией по Тихонову, а если $\\frac{\\lambda}{2m} \\sum_{j=1}^n\\left|\\theta_j\\right|$ — методом LASSO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Метод опорных векторов (SVM)\n",
    "----------------------------\n",
    "\n",
    "Формально метод опорных векторов очень похож на логистическую регрессию с тем отличием, что логарифмические слагаемые в функции потерь заменены кусочно-линейными аппроксимациями $R_1$ и $R_0$. Также регуляризационный коэффициент перенесён к первому слагаемому, что не влияет на результат, так как не смещает $\\arg \\min J(\\theta)$.\n",
    "\n",
    "В итоге получаем:\n",
    "$$\n",
    "J(\\theta) = C \\sum_{i=1}^m \\Big[ \n",
    "    y^{(i)} R_1 (\\theta^T x^{(i)}) + (1 - y^{(i)}) R_0 (\\theta^T x^{(i)})\n",
    "\\Big] + \\sum_{j=1}^n\\theta_j^2.\n",
    "$$\n",
    "\n",
    "Геометрически идея SVM заключается в поиске такой разделяющей прямой, чтобы зазор между двумя классами был максимальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод k ближайших соседей (kNN)\n",
    "-------------------------------\n",
    "\n",
    "Для того, чтобы применять метод kNN необходимо ввести понятие расстояния между объектами. Для вычисления расстояния можно использовать произвольные метрики. Часто используют метрики L2 (эвклидово расстояние, норма $l_2$) и L1 (манхэттенское расстояние, норма $l_1$). Например, для метрики L2 расстояние вычисляется следующим образом:\n",
    "$$\n",
    "\\left\\|x^{(i)} - x^{(j)}\\right\\|_2 = \\sqrt{ \\sum_{k=0}^n \\left( x^{(i)}_k - x^{(j)}_k \\right) ^ 2 } = \n",
    "\\sqrt{\\left(x^{(i)} - x^{(j)}\\right)^T\\left(x^{(i)} - x^{(j)}\\right)}.\n",
    "$$\n",
    "\n",
    "Идея метода в следующем. Для того, чтобы определить класс нового объекта, в обучающей выборке находят k ближайших к нему объектов — ближайшх соседей. Класс объекта принимается равным классу, к которому относится большинство ближайших соседей.\n",
    "\n",
    "Метод плохо работает, если обучающая выборка очень большая, так как поиск может стать очень долгим. К преимуществам метода относится то, что он не требует никаких предварительных вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека scikit-learn\n",
    "-----------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Популярная библиотека scikit-learn содержит реализации многих методов машинного обучения. В том числе логистической регрессии (`sklearn.linear_model.LogisticRegression`), классификации на основе SVM (`sklearn.svm.SVC`) и kNN (`sklearn.neighbors.KNeighborsClassifier`).\n",
    "\n",
    "Работа с классификаторами в библиотеке единообразна и отличается лишь строкой, в которой создаётся конкретный классификатор. Рассмотрим работу с библиотекой на примере SVM.\n",
    "\n",
    "Подключение библиотеки и извлечение классификатора SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве исходных данных возмём случайно сгенерированные точки на плоскости, группирующиеся в два кластера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# Генерируем случайные данные\n",
    "X, y = make_blobs(n_samples=100,  # число точек\n",
    "                  centers=2,      # число кластеров\n",
    "                  n_features=2)   # число признаков\n",
    "\n",
    "plt.scatter(X[:, 0], X[:,1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим классификатор. Большую часть параметров можно не задавать, укажем только значение коэффициента регуляризации $C$ и тип классификатора — линейный (`linear`). Кроме линейного можно использовать, например, радиально-базисные функции (`rbf`) или многочлены (`poly`). Для многочленов нужно указывать степень (параметр `degree`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0,             # коэффициент регуляризации\n",
    "          kernel='linear')   # вид ядра (линейное)\n",
    "\n",
    "print(clf)                   # Вывод значения параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У всех классификаторов есть метод `fit`, выполняющий обучение. Пользоваться им очень легко:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, чтобы воспользоваться уже обученным классификатором, вызовем метод `predict`. Он принимает на входе матрицу, строки которой содержат признаки объектов, и возвращает вектор с классами. Определим класс точки $(0, 1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(np.array([[0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверить точность обучения можно с помощью метода `score`. Он возвращает долю правильно классифицированных объектов. Проверим точность классификатора на исходной выборке (хотя всегда проверяют на других данных):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбить исходную выборку случайным образом на две части — обучающую и тестовую — можно функцией `train_test_split` (впрочем, ниже мы рассмотрим более эффективный метод — перекрёстную проверку). Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,             # Данные\n",
    "    test_size=0.33    # Объём тестовой выборки (доля объектов)\n",
    "    )\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если точность на обучающей выборке намного выше точности на тестовой, скорее всего имеет место переобученность модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим разделяющую кривую. Для этого вычислим значения классов на сетке с малым шагом $h$ и отобразим границу в виде линии. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01 # Шаг сетки\n",
    "\n",
    "# Строим сетку\n",
    "xx, yy = np.meshgrid(np.arange(min(X[:, 0]), max(X[:, 0]), h),\n",
    "                     np.arange(min(X[:, 0]), max(X[:, 0]), h))\n",
    "\n",
    "\n",
    "# Вычисляем класс для каждой точки сетки\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Отображаем данные и границу классов\n",
    "plt.scatter(X[:, 0], X[:,1], c=y)\n",
    "plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных\n",
    "-----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения разных признаков могут иметь сильно различающиеся масштабы. Это может существенно повлиять на точность обучения. Поэтому данные предварительно нормируют при помощи деления на характерную (среднюю) величину и смещения к нулю. Это можно делать автоматически средствами библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)            # scaler хранит параметры нормировки\n",
    "X_train_transformed = scaler.transform(X_train)   # нормируем с теми же параметрами\n",
    "\n",
    "clf = SVC().fit(X_train_transformed, y_train)     # создаём и обучаем классификатор\n",
    "\n",
    "X_test_transformed = scaler.transform(X_test)     # нормируем тестовую выборку\n",
    "clf.score(X_test_transformed, y_test)             # оцениваем результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перекрёстная проверка\n",
    "---------------------\n",
    "\n",
    "При оценке качества модели важно оставить часть данных для тестирования. Хотя тестовая выборка и выделяется обычно случайно, разбиение может оказаться неудачным и переобученность модели окажется незамеченной. Более эффективно применение перекрёстной проверки (crossvalidation).\n",
    "\n",
    "При перекрёстной проверке данные разбиваются на несколько равных частей (пять — хороший выбор). Первая часть оставляется для тестирование, а на остальных модель обучается. Затем вторая оставляется для тестирования и так далее. В итоге получаем ошибки для каждого разбиения. Итоговую ошибку можно вычислить как среднее из них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf,  # Классификатор\n",
    "                         X, y, # Все данные\n",
    "                         cv=5  # Количество частей\n",
    "                        )\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка доверительного интервала для доли верно распознаваемых объектов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Точность: %0.2f (±%0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ирисы Фишера\n",
    "------------\n",
    "\n",
    "Один из стандартных наборов данных для проверки алгоритмов классификации — ирисы Фишера. Он был подготовлен в 1936 году известным математиком и биологом Р. Фишером и содержит информацию о 150 экземплярах ирисов следующих видов:\n",
    "\n",
    "- Ирис щетинистый (Iris setosa),\n",
    "- Ирис виргинский (Iris virginica) и\n",
    "- Ирис разноцветный (Iris versicolor).\n",
    "\n",
    "Каждый вид представлен 50 экземплярами. Признаки, входящие в набор:\n",
    "\n",
    "- длина наружной доли околоцветника (sepal length);\n",
    "- ширина наружной доли околоцветника (sepal width);\n",
    "- длина внутренней доли околоцветника (petal length);\n",
    "- ширина внутренней доли околоцветника (petal width).\n",
    "\n",
    "В библиотеке scikit-learn есть возможность автоматически загрузить этот набор данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание\n",
    "-------\n",
    "\n",
    "1. Попробуйте решить рассмотренную выше задачу для случая трёх и более классов. В этой и последующих задачах выполните нормализацию перед обучением и оцените результаты с помощью перекрёстной проверки. Посмотрите, как влияет параметр C на решение. Попробуйте использовать полиномиальный классификатор ('poly') или радиально-базисные функции ('rbf').\n",
    "2. Решите задачу классификации ирисов Фишера с помошью логистической регрессии с использованием scikit-learn и непосредственным программированием. (Для этого может пригодиться код из предыдущей лабораторной работы.)\n",
    "3. Решите задачу классификации ирисов Фишера методом kNN с использованием scikit-learn и непосредственным программированием. Как на результат влияет параметр k?\n",
    "4. Решите задачу классификации ирисов Фишера методом SVM с использованием scikit-learn. Как на результат влияет параметр C?\n",
    "5. Сформулируйте и запишите выводы. Как влияет регуляризация на результаты обучения? Можно ли обучить классификатор так, чтобы он не давал ошибок? Какой метод показал себя лучше: логистическая регрессия, метод опорных векторов или метод kNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
